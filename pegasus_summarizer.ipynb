{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pegasus Summarizer \n",
    "\n",
    "This notebook is utilizing the Pegasus Summarizer from Google to test out the Summarization of the model created by Google. This notebook doesn't necessary investigate into the complexities behind the model rather test run the model to have a functional baseline that returns an output. We will be looking to take a blob of text and run it through the model to get an understanding if the model is return an abstractive summary or an extractive summary. We will explore more into the model's capabilities as we expand on this repo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CUDA and Model name\n",
    "\n",
    "Set the model name to specified summarizer.  If you are looking to use another pegasus model, change the model name. For this notebook, we are explicitely using the Pegasus Summarizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'google/pegasus-xsum'\n",
    "torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download the Model \n",
    "\n",
    "We will be downloading the model in this section.  This is take about 5-10 minutes depending on your connection. This will be downloaded from the public internet. We will also set the model to use the specified CUDA hardware (if available).  Otherwise, the model will be using a CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "model = PegasusForConditionalGeneration.from_pretrained(model_name).to(torch_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text to Summarize\n",
    "\n",
    "Ironically, we will be summarizing a summary of a Christopher Nolan's movie **Dunkirk**. So let's create a variable that we will use to pass it along into the model. The source text needs to be a list format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = [\"In May 1940, Germany advanced into France, trapping Allied troops on the beaches of Dunkirk. Under air and ground cover from British and French forces, troops were slowly and methodically evacuated from the beach using every serviceable naval and civilian vessel that could be found. At the end of this heroic mission, 330,000 French, British, Belgian and Dutch soldiers were safely evacuated.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "393"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(plot[0]) ## The length of the text we are currently passing into the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Preparing the Model\n",
    "\n",
    "Let's start preparing the model in this step. From the tokenizers, we will be invoking the sequence to sequence and will be passing in the movie plot summary we just defined above.  Some of the critical parameters to note here: Truncation is being set to **true**, and padding is being set to **longest**.  If you are looking to summarize very large text, it might be ideal to might the longest length of the text, and truncate/pad if needed. Otherwise, you may be chopping off text that maybe critical to the summary of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = tokenizer.prepare_seq2seq_batch(\n",
    "    plot, \n",
    "    truncation=True,\n",
    "    padding='longest',\n",
    "    return_tensors='pt'\n",
    ").to(torch_device) # Pass these parameters into the torch device "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate a Batch Result \n",
    "\n",
    "Let's generate a batch result and then we will grab the summary from the provided output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "translated = model.generate(**batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0,   139,  7467,   113, 69119,   140,   156,   113,   109,  1458,\n",
       "         10083, 10931,   113,   894,  1981,  2508,   107,     1]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translated  ## Returns the resulting tensors from the batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = tokenizer.batch_decode(translated, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The Battle of Dunkirk was one of the bloodiest battles of World War Two.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output ## Summary Output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "peg",
   "language": "python",
   "name": "peg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
